[
  {
    "path": "r_posts/2024-08-13-network-analysis/",
    "title": "A Study of Social Network Analysis on Hazards Risk Reduction",
    "description": "Keywords: Network analysis, network application, visualization.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2024-08-22",
    "categories": [],
    "contents": "\r\nThis post introduces one way to visualize networks using the package igraph. It is fairly straightforward, the steps we need to take are:\r\nAttach the packages.\r\nImport nodes.\r\nImport edges.\r\nEstablish nodes and edges and assign weights based on contextualized studies.\r\nCreate the net.\r\nCheck if you have the nodes and edges connected based on your needs.\r\nVisualize it using the plot function, and adjust the parameters so that the network looks aesthetically nice.\r\nEssentially, the content in this network project is used to analyze local hazards risk reduction focusing on stakeholders’ collaborative activities and co-worked experience. The network of plans illustrate the extent that existing plan documents are integrated for informed decision making. The network of people demonstrate the types of agency, the frequency of co-working experience, and the depth of conversation in local risk reduction.\r\nFind more information here for the published article if you are interested. Below are step-by-step network visualization.\r\nAttach packages\r\n\r\n\r\n#install.packages(\"igraph\")\r\n#install.packages(\"igraphdata\")\r\nlibrary(igraph)\r\n#library(igraphdata)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(tidyverse)\r\n#install.packages(stringr)\r\nlibrary(ITNr)\r\nlibrary(igraphdata)\r\nlibrary(RColorBrewer)\r\n\r\n\r\nImport edges and nodes\r\nYou may find examples of edges and nodes in txt format in source file.\r\n\r\n\r\nedges <- read.csv(\"link.csv\")\r\n\r\n\r\n\r\n\r\nnodes <- read.csv(\"node.csv\")\r\n\r\n\r\n\r\n\r\n# Check the number of nodes\r\nnrow(nodes); length(unique(nodes$id))\r\n\r\n[1] 18\r\n[1] 18\r\n\r\n\r\n\r\n# Check the number of unique edges\r\nnrow(edges); nrow(unique(edges[,c(\"from\", \"to\")]))\r\n\r\n[1] 136\r\n[1] 126\r\n\r\n\r\n\r\n# We will collapse all links of the same type between the same two nodes by summing their weights, using aggregate() by “from”, “to”, & “type”\r\nlinks <- aggregate(edges[,3], edges[,-3], sum)\r\nlinks <- links[order(links$from, links$to),]\r\ncolnames(links)[4] <- \"weight\"\r\nrownames(links) <- NULL\r\n\r\n\r\n\r\n\r\nnet <- graph_from_data_frame(d=links, vertices=nodes, directed=F)\r\n\r\n\r\n\r\n\r\n# If you have extra attributes in your edge and vertices as appeared in your data collection, \r\n#you may add them to the edge and vertices. Four instance, the vertex here shows the stakeholders' agency names. \r\n#The type of the edge also represents the type of meeting that get stakeholders connected. \r\n\r\nE(net)$type\r\n\r\n  [1] \"lup\" \"rec\" \"mit\" \"rec\" \"mit\" \"rec\" \"mit\" \"res\" \"mit\" \"res\"\r\n [11] \"mit\" \"mit\" \"lup\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\"\r\n [21] \"res\" \"res\" \"res\" \"lup\" \"rec\" \"rec\" \"mit\" \"rec\" \"mit\" \"mit\"\r\n [31] \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\"\r\n [41] \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"mit\" \"res\" \"mit\" \"mit\" \"mit\"\r\n [51] \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n [61] \"res\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\"\r\n [71] \"res\" \"res\" \"res\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\"\r\n [81] \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\"\r\n [91] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[101] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[111] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[121] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[131] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n\r\nV(net)$agency #vertex attribute \"agency name\"\r\n\r\n [1] \"CoPlanning\"    \"CoLegislative\" \"CoBudget\"      \"CoPublicWork\" \r\n [5] \"CoEM\"          \"StateEM\"       \"CoStormwater\"  \"CoExecutive\"  \r\n [9] \"CoInfo\"        \"CoBuilding\"    \"Insurance\"     \"DisasterSpec\" \r\n[13] \"HigherEdu\"     \"NOAA\"          \"USACE\"         \"StateResource\"\r\n[17] \"DisasterSpec2\" \"Consultant1\"  \r\n\r\n#find specific nodes \r\nV(net)[agency == \"CoPlanning\"]\r\n\r\n+ 1/18 vertex, named, from 438c3e6:\r\n[1] s01\r\n\r\n#specific planning type\r\nE(net)[type ==\"lup\"]\r\n\r\n+ 3/136 edges from 438c3e6 (vertex names):\r\n[1] s01--s02 s01--s10 s02--s10\r\n\r\n\r\n\r\n# Scale the layout by 5 to increase distance between nodes\r\nlayout <- layout_with_fr(net) * 5  \r\n\r\nplot(net, \r\n     layout = layout,\r\n     edge.color=\"grey\",\r\n     vertex.color=\"orange\", \r\n     vertex.frame.color=\"#ffffff\",\r\n     vertex.label.cex=.6,\r\n     vertex.label=V(net)$agency,\r\n     vertex.label.color=\"black\")\r\n\r\n\r\n\r\n\r\n\r\n# This plot is to visualize actors based on type (e.g., county, state, fed, private, nonprofit, private)\r\n# set attributes is to add them to the igraph object\r\n# Generate colors based on agency type:\r\ncolrs <- c(\"antiquewhite2\", \"grey\", \"gold\", \"coral\",\"aquamarine\")\r\n\r\nV(net)$color <- colrs[V(net)$agency.type]\r\nE(net)$color <- as.factor(links$type)\r\n# Compute node degrees (#links) and use that to set node size:\r\ndeg <- degree(net, mode=\"all\")\r\nV(net)$size <- deg*3\r\n\r\n# use the influence weight for nodes:\r\nV(net)$size <- V(net)$influence*2.2\r\n\r\n# Setting them to NA will render no labels:\r\nV(net)$label <- NA\r\n\r\n# Set edge width based on weight:\r\nE(net)$width <- E(net)$weight\r\n#change arrow size and edge color:\r\n#E(net)$arrow.size <- .2\r\nE(net)$edge.color <- \"azure4\"\r\nE(net)$width <- 1+E(net)$weight*.8\r\n\r\n# Scale the layout to increase edge length\r\nlayout <- layout_with_fr(net)\r\nscaled_layout <- layout * 2  \r\n\r\nplot(net,\r\n     vertex.frame.color=\"#ffffff\",\r\n     vertex.label.cex=.6, \r\n     vertex.label=V(net)$agency,\r\n     vertex.label.color=\"black\",\r\n     layout = scaled_layout)\r\n\r\n# Adjustment to the legend\r\nlegend(\"right\", c(\"Local Agency\",\"State Agency\", \"Federal Agency\", \"Private Sector\", \"Nonprofit\"), pch=21,\r\n       col=\"#777777\", pt.bg=colrs, pt.cex=2, cex=.8, bty=\"n\", ncol=1)\r\n\r\n\r\n\r\n\r\n\r\n# Calculate the degree for each node\r\ndeg_1 <- degree(net)\r\nhead(deg_1)\r\n\r\ns01 s02 s03 s04 s05 s06 \r\n 31   2   3  13  13  26 \r\n\r\n# Calculate the closeness centrality for each node\r\nclo_1 <- closeness(net, vids = V(net), normalized = F)\r\nhead(clo_1)\r\n\r\n       s01        s02        s03        s04        s05        s06 \r\n0.05555556 0.03125000 0.03225806 0.04347826 0.04166667 0.05263158 \r\n\r\n# Calculate the betweenness centrality for each node\r\nbet_1 <- betweenness(net, normalized = F)\r\nhead(bet_1)\r\n\r\n      s01       s02       s03       s04       s05       s06 \r\n27.300000  0.000000  0.000000  3.041667  1.850000  9.241667 \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2024-08-13-network-analysis/prac3_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2024-08-31T11:51:28-05:00",
    "input_file": "prac3.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2022-08-22-Bayesian/",
    "title": "A Bayes Application and made up examples",
    "description": "Keywords: Bayes Theorem, Bayesian application, Bayes interpretation.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [],
    "contents": "\r\nThis post is my own practice and preparation of using/applying\r\nBayesian regression. Material and example codes are from John\r\nMyles White.\r\nThe first step is to create a set of data that fulfills the following\r\nrequirements: Gaussian distributed with equal variance and a meang which\r\nis a linear function of the predictors.\r\n\r\n    b_length    b_mass\r\n1 -2.6564554  5.712696\r\n2 -2.4404669  9.119517\r\n3 -1.7813084 17.362433\r\n4 -1.7631631  9.323737\r\n5 -1.3888607 18.636169\r\n6 -0.6399949 15.015008\r\n\r\nNow, before analyzing, convert to the list format. The list will have\r\nthree elements, body mass, body length, and a number that indicates the\r\nsample size.\r\nWrite JAGS model. Create the model with JAGS code, there are\r\ndifferences between R and JAGS code: JAGS use dnorm for distribution’s\r\nprecision (1/variance). Tilde (~) signifies that object on the left is\r\nrandom variable distributed according to the distribution on the right.\r\nAlpha and Beta are intercepts of linear relationships.\r\n\r\n\r\n\r\nThe initial parameter values for the MCMC. The parameters are chosen\r\nwhose posterior distributions will be reported and runs the model in\r\nJAGS.\r\nThree MCMC chanins with 12000 iterations each, and discards the first\r\n2000 values, because these first 2000 values highly depend on initial\r\nvalues.\r\nn.thin means only every 10th iteration is saved and the rest\r\ndiscarded.\r\nModel output. The 2.5% and 97.5% quantile range are the 95% credible\r\ninterval for each parameter. For the 95% credible interval, there is a\r\n95% probability that the true parameter lies within this range.\r\nThe last two columns are convergence diagnostics. n.eff is a number\r\nsmaller than or equal to the number of samples saved from the chains\r\n(3*(12000-2000)/10). The higher autocorrelation in the saved samples\r\nindicate a smaller effective sample size. Rhat indicates how well the\r\nthree Markov chains are mixed. Ideal Rhat is close to 1. Large Rhat\r\nvalues mean that chains are not mixed properly and posterior estimates\r\ncannot be trusted.\r\n\r\n\r\nfit_lm1\r\n\r\nInference for Bugs model at \"/var/folders/xj/c74jv4d928z1rmtv4_vchj740000gn/T//RtmpKVxs4V/model158f583cb405.txt\", fit using jags,\r\n 3 chains, each with 12000 iterations (first 2000 discarded), n.thin = 10\r\n n.sims = 3000 iterations saved\r\n      mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\r\nalpha  29.079   1.038 27.063 28.408 29.101 29.761 31.086 1.002  1800\r\nbeta    9.736   0.845  8.044  9.199  9.744 10.294 11.342 1.001  3000\r\nsigma   5.586   0.812  4.275  5.015  5.485  6.074  7.371 1.001  3000\r\n\r\nFor each parameter, n.eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\r\n\r\nThe traceplot is used to visualize how well the chains have mixed. In\r\nthis case, chains are nicely mixed\r\n\r\n\r\n\r\nOther ways to demonstrate posterior distributions of parameters\r\ngraphically.\r\n\r\n\r\n\r\nVisualize in MCMC format\r\n\r\n\r\n\r\nIf we change parameters and plot the mcmc visuals, we want to see the\r\ndifferences when altering.\r\nCreate\r\na Catogorical variable (sex) and adding to the jags model\r\n\r\n$b_mass\r\n [1] 19.309795 21.757625  8.176761 18.101959  6.869465 30.544474\r\n [7] 26.567582 16.759391 28.439164 15.914424 32.568771 21.647253\r\n[13] 29.606252 28.577468 35.155978 38.940458 23.314439 29.641170\r\n[19] 33.889166 27.456285 22.908149 27.540421 26.500509 34.466663\r\n[25] 37.523150 35.058236 34.596614 30.236815 41.302396 30.297417\r\n[31] 34.570252 35.918447 33.942750 35.541939 39.130491 39.304575\r\n[37] 39.992016 34.129303 34.527288 32.001647 38.439367 39.583496\r\n[43] 57.643543 36.390552 44.395866 36.972887 38.228475 48.889866\r\n[49] 45.201041 50.997405\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\n\r\nTest\r\nof increasing the range of data (by increasing the standard deviation 10\r\ntimes)\r\n\r\n\r\nlm2_jags <- function(){\r\n  # Likelihood:\r\n  for (i in 1:N){\r\n    b_mass[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)\r\n    mu[i] <- alpha[1] + sex[i] * alpha[2] + \r\n      (beta[1] + beta[2] * sex[i]) * b_length[i]\r\n  }\r\n  # Priors:\r\n  for (i in 1:2){\r\n    alpha[i] ~ dnorm(0, 0.01) \r\n    beta[i] ~ dnorm(0, 0.01)  \r\n  }\r\n  sigma ~ dunif(0, 1000) \r\n  tau <- 1 / (sigma * sigma)\r\n}\r\n\r\n# ---------\r\ninit_values <- function(){\r\n  list(alpha = rnorm(2), beta = rnorm(2), sigma = runif(1))\r\n}\r\n\r\nparams <- c(\"alpha\", \"beta\", \"sigma\")\r\n\r\nfit_lm2 <- jags(data = jagsdata_s2, \r\n                inits = init_values, \r\n                parameters.to.save = params, \r\n                model.file = lm2_jags, n.chains = 3, \r\n                n.iter = 12000, n.burnin = 2000, n.thin = 10, DIC = F)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n#--------\r\nlm2_mcmc <- as.mcmc(fit_lm2)\r\nplot(lm2_mcmc)\r\n\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but low variance.\r\n\r\n$b_mass\r\n [1] 42.27916 44.07903 20.24780 38.44588 15.68528 50.69550 45.73416\r\n [8] 23.60080 45.99189 19.97139 49.92215 25.28078 46.52624 31.62210\r\n[15] 51.85007 55.23480 25.46678 31.44646 35.42236 43.30904 24.30209\r\n[22] 43.31223 42.01626 49.86663 52.84152 35.53153 34.91018 45.12845\r\n[29] 40.27240 28.48178 32.54891 49.61999 31.66550 33.24145 52.61563\r\n[36] 52.40599 53.08417 30.85106 31.00310 44.72716 33.26385 33.51012\r\n[43] 68.72893 29.78999 37.54107 29.75238 48.69391 58.20429 35.10892\r\n[50] 59.13747\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but large variance.\r\nThe density of sigma shifted significanly.\r\n\r\n$b_mass\r\n [1]  53.8990908  59.4426007  29.5231515  20.7303588  -6.3103379\r\n [6]  80.9496405  50.8925931  25.3696005  43.5739562  -3.9151883\r\n[11]  62.1620864  20.9379797  42.8711028  50.2890263  68.2855351\r\n[16]  83.0771246  15.9433063  44.4534275  63.2445684  21.0932663\r\n[21]   7.0862415  20.6774554  12.8319806  51.4662785  65.9056104\r\n[26]  59.5508392  55.8052067  25.0642743  77.2420405  15.1463068\r\n[31]  34.6591869  41.1748751  29.2184963  37.0053126  54.9988444\r\n[36]  51.9041366  55.2456198  21.1423585  20.9187584  11.5051759\r\n[41]  25.6171744  23.2571170 122.7667536   2.5476609  40.2861977\r\n[46]  -0.1201204  19.2851945  60.6983335  15.1761402  59.1010162\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2022-08-22-Bayesian/2022-08-22-Bayes_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2016-04-20-time-series/",
    "title": "Time Series Analysis with Examples",
    "description": "This is a project from undergraduate studies, I post it for getting familiar with some basic concepts of time series analyses.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\r\nMotivation\r\nMovies have been a source of entertainment for people over years.\r\nSummertime superhero blockbusters typically bring excitement to an\r\notherwise calm time. Theaters can rake in huge sums of money on these\r\ntypes of movies. In particular the movie Iron Man 3 released on May 3r\r\n2013 provides a great example of this phenomenon. The box of ticket\r\nsales of this movie have been analyzed to ft the best time series plot\r\npossible to attempt to approximate future movie ticket sales for movies\r\nof this type.\r\nMethodology\r\nTo complete the analysis data on ticket sales were gathered from https://www.the-numbers.com/ for Iron Man 3. Daily and\r\nweekly per-theater ticket sales were collected and fitted to several\r\ntime series models. The models included moving average single\r\nexponential Holt-t Winters simple linear regression and multiple linear\r\nregression. Each model was judged based on three accuracy measures: MAD\r\nMAPE and MSD. The best model for each type of data (daily and weekly)\r\nwas selected and recommended to be used in the prediction of future\r\nmovie releases.\r\nResults and Analysis\r\nFor the Time Series Plot of Weekly Per Theater Ticket Sales (Figure\r\n1)r there is a decreasing trend. The value of per-theater ticket sales\r\ndecreases until a certain point before leveling out about 5 weeks after\r\nrelease. There is no cyclical or seasonal effect that can be found from\r\nthe time series plot and the plot looks fairly smooth. For the week\r\n2013/5/24 and the week 2013/08/16r there were slight increasing in both\r\nweeks the reason of increasing in May could be the end of semester and\r\nstudents throwing parts after finals. In August good weather and nearing\r\nthe end of summer breaks may play a factor.\r\n The Time Series Plot of Daily\r\nTicket Sales (Figure 2), overall there is a decreasing trend starting\r\nfrom May and almost leveled off in June. There is a seasonal variation\r\ncan be found for daily because people are more likely to go to movies on\r\nweekends than weekdays. The seasonal variation shows a 7-day repeating\r\npattern with 3-day peak, representing Friday, Saturday and Sunday, which\r\nmore people prefer to go to movies on.\r\nFigure 2Weekly\r\nFor the weekly ticket sales, analysis was conducted for the moving\r\naverage, exponential smoothing, Holt-Winters, and simple linear\r\nregression models with accuracy measures shown in Table 1. Based on\r\nthese accuracy measures, the moving average was by far the best model,\r\nas seen in Figure 3. All three accuracy measures were the lowest of any\r\nmodel. The moving average model was fit with an MA length of 4 and the\r\nmoving averages were centered.\r\nThe exponential model was fit using an alpha of 0.8. Initial attempts\r\nto optimize ARIMA to find an ideal alpha did not work, as Minitab threw\r\nan error code. After adjusting the alpha value through several\r\niterations, a value of 0.8 was selected as the best. With this value,\r\nthe model was still objectively worse than the moving average in all\r\naccuracy measures.\r\nFigureFigure 3The same process was used (multiple iterations to find the best\r\nvalues) for the Holt-Winters method with a final model using an alpha of\r\n0.8, a gamma of 0.3, and a delta of 0. The delta was selected to be 0\r\nbecause the weekly ticket sales showed no seasonal trend. This model was\r\nagain worse than the moving average.\r\nFinally, a simple linear regression was constructed. The model fit\r\nthe data to the equation TicketSales = 21943 – 1715*(days after\r\nrelease). This model was calculated to have worse accuracy measures than\r\nthe moving average. A multiple regression, using seasons as categorical\r\nvariables, was not conducted as there was no seasonal trend for the\r\nweekly dataset. As such, trying to force a fit on that data set would\r\nnot be effective.\r\nOverall, the moving average model was best for the weekly,\r\nper-theater ticket sales for Iron Man 3. This model had the best\r\naccuracy measures and makes sense. The sales values level off very\r\nquickly and stay effectively horizontal for most of the plot, which the\r\nmoving average model handles well. Additionally, none of the models\r\nhandled the first few data points very well, and the moving average had\r\nthe benefit of not starting to fit points until the third week, due to\r\nthe nature of the analysis.\r\nAs a secondary option, the Holt-Winters model also fit the data\r\nreasonably well (Figure 4). While there was a large overprediction for\r\nthe first week, and some underprediction for the following several\r\nweeks, the model did a good job tracking the trend and pattern of the\r\nticket sales. The sort of decay pattern is matched well, despite not\r\nhaving any seasonal effect (which the Holt-Winters tries to model).\r\nAdditionally, this model had the second best MAD, and MSD.\r\nFigure 4Daily\r\nFor daily ticket sales, the analysis found slightly different\r\nresults. As mentioned above, the daily ticket sales showed a definite\r\nseasonal pattern, and this makes sense. Weekends will have much higher\r\nbox office sales than weekdays. As such, a model that could take\r\nseasonality into account would likely be much better.\r\nThis tended to show true as the Holt-Winters method had the best\r\naccuracy measures in two out of three cases (the MAPE and MAD were best\r\nfor Holt-Winters). The Holt-Winters model, as seen in Figure 5, was able\r\nto handle the seasonality, which the moving average and single\r\nexponential could not do. They both had accuracy measures much higher\r\nthan the Holt-Winters method.\r\nFigureFigure 5Regression models were also fit with a single regression model having\r\nan equation of TicketSales = 4821 - 99.1(days since release). This model\r\nfit very poorly, however, due to the seasonality of the data a multiple\r\nregression model was also fit. Days since release was used as a\r\ncontinuous predictor with days of the week used as categorical\r\npredictors. The best equation was found by including a second-order term\r\nfor days since release and using an interaction between days since\r\nrelease and days of the week. The actual equation was found to be the\r\nfollowing, based on day of the week:\r\nSunday Daily Ticket \\[Sales = 9544 - 413.8\r\n(Days Since Release) + 4.530 (Days Since Release)^2 \\]\r\nMonday Daily Ticket \\[Sales = 5067 - 324.1\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nTuesday Daily Ticket \\[Sales = 5216 -\r\n331.3 (Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nWednesday Daily Ticket \\[Sales = 4474 - 304.0\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\] Thursday\r\nDaily Ticket \\[Sales = 4671 - 310.7 (Days\r\nSince Release) + 4.530 (Days Since Release)^2\\] Friday Daily\r\nTicket \\[Sales = 10803 - 447.1 (Days Since\r\nRelease) + 4.530 (Days Since Release)^2\\] Saturday Daily Ticket\r\n\\[Sales = 12051 - 464.4 (Days Since Release)\r\n+ 4.530 (Days Since Release)^2\\]\r\nThis regression equation had accuracy measures calculated manually to\r\nbe 143; 977; and 1,825,432 for the MAPE, MAD, and MSD, respectively.\r\nThese measures were objectively worse than those for other models, so\r\nthe multiple regression model was not used.\r\nThe moving average was selected as an alternate model (Figure 6), as\r\nits accuracy measures were generally good. It had the best MSD, second\r\nbest MAD, and third MAPE of all the models. While this model did not do\r\ngreat in the first week, it steadied out and modeled the relative\r\nflatness of later ticket sales pretty well.\r\nFigure 6Conclusion\r\nBy using the data collected from http://www.thenumbers.com of the movie The Iron Man 3,\r\ndifferent times series models were generated including moving average,\r\nsingle exponential smoothing, Holt-Winters, simple linear regression,\r\nand multiple linear regression. For the weekly ticket sales, based on\r\nthe analysis among all models, a moving average should be chosen because\r\nit had the best accuracy measures (MAPE, MAD, and MSD). For the daily\r\nticket sale, there is not just a trend, but a seasonal pattern within\r\nthe time series. So, the Holt-Winters method did a good job on\r\npredicting ticket sales. With this information, it is fair to assume\r\nthat in the prediction of future movie ticket sales, a moving average\r\nwould be best to determine weekly ticket sales and a Holt-Winters model\r\nwould be best for daily ticket sales. Unfortunately, neither model did a\r\ngreat job of predicting opening day/week sales, but the models steadied\r\nafter that. So, in conclusion the models mentioned above are useful in\r\npredicting ticket sales for summer blockbuster superhero movies, and may\r\nbe useful in predicting sales for all movies; however, the prediction is\r\nlimited to times beyond the opening week.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {}
  }
]
