[
  {
    "path": "r_posts/2022-08-22-Bayesian/",
    "title": "A Bayes Application and made up examples",
    "description": "Keywords: Bayes Theorem, Bayesian application, Bayes interpretation.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [],
    "contents": "\r\nThis post is my own practice and preparation of using/applying\r\nBayesian regression. Material and example codes are from John\r\nMyles White.\r\nThe first step is to create a set of data that fulfills the following\r\nrequirements: Gaussian distributed with equal variance and a meang which\r\nis a linear function of the predictors.\r\n\r\n    b_length    b_mass\r\n1 -2.6564554  5.712696\r\n2 -2.4404669  9.119517\r\n3 -1.7813084 17.362433\r\n4 -1.7631631  9.323737\r\n5 -1.3888607 18.636169\r\n6 -0.6399949 15.015008\r\n\r\nNow, before analyzing, convert to the list format. The list will have\r\nthree elements, body mass, body length, and a number that indicates the\r\nsample size.\r\nWrite JAGS model. Create the model with JAGS code, there are\r\ndifferences between R and JAGS code: JAGS use dnorm for distribution’s\r\nprecision (1/variance). Tilde (~) signifies that object on the left is\r\nrandom variable distributed according to the distribution on the right.\r\nAlpha and Beta are intercepts of linear relationships.\r\n\r\n\r\n\r\nThe initial parameter values for the MCMC. The parameters are chosen\r\nwhose posterior distributions will be reported and runs the model in\r\nJAGS.\r\nThree MCMC chanins with 12000 iterations each, and discards the first\r\n2000 values, because these first 2000 values highly depend on initial\r\nvalues.\r\nn.thin means only every 10th iteration is saved and the rest\r\ndiscarded.\r\nModel output. The 2.5% and 97.5% quantile range are the 95% credible\r\ninterval for each parameter. For the 95% credible interval, there is a\r\n95% probability that the true parameter lies within this range.\r\nThe last two columns are convergence diagnostics. n.eff is a number\r\nsmaller than or equal to the number of samples saved from the chains\r\n(3*(12000-2000)/10). The higher autocorrelation in the saved samples\r\nindicate a smaller effective sample size. Rhat indicates how well the\r\nthree Markov chains are mixed. Ideal Rhat is close to 1. Large Rhat\r\nvalues mean that chains are not mixed properly and posterior estimates\r\ncannot be trusted.\r\n\r\n\r\nfit_lm1\r\n\r\nInference for Bugs model at \"/var/folders/xj/c74jv4d928z1rmtv4_vchj740000gn/T//RtmpKVxs4V/model158f583cb405.txt\", fit using jags,\r\n 3 chains, each with 12000 iterations (first 2000 discarded), n.thin = 10\r\n n.sims = 3000 iterations saved\r\n      mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\r\nalpha  29.079   1.038 27.063 28.408 29.101 29.761 31.086 1.002  1800\r\nbeta    9.736   0.845  8.044  9.199  9.744 10.294 11.342 1.001  3000\r\nsigma   5.586   0.812  4.275  5.015  5.485  6.074  7.371 1.001  3000\r\n\r\nFor each parameter, n.eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\r\n\r\nThe traceplot is used to visualize how well the chains have mixed. In\r\nthis case, chains are nicely mixed\r\n\r\n\r\n\r\nOther ways to demonstrate posterior distributions of parameters\r\ngraphically.\r\n\r\n\r\n\r\nVisualize in MCMC format\r\n\r\n\r\n\r\nIf we change parameters and plot the mcmc visuals, we want to see the\r\ndifferences when altering.\r\nCreate\r\na Catogorical variable (sex) and adding to the jags model\r\n\r\n$b_mass\r\n [1] 19.309795 21.757625  8.176761 18.101959  6.869465 30.544474\r\n [7] 26.567582 16.759391 28.439164 15.914424 32.568771 21.647253\r\n[13] 29.606252 28.577468 35.155978 38.940458 23.314439 29.641170\r\n[19] 33.889166 27.456285 22.908149 27.540421 26.500509 34.466663\r\n[25] 37.523150 35.058236 34.596614 30.236815 41.302396 30.297417\r\n[31] 34.570252 35.918447 33.942750 35.541939 39.130491 39.304575\r\n[37] 39.992016 34.129303 34.527288 32.001647 38.439367 39.583496\r\n[43] 57.643543 36.390552 44.395866 36.972887 38.228475 48.889866\r\n[49] 45.201041 50.997405\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\n\r\nTest\r\nof increasing the range of data (by increasing the standard deviation 10\r\ntimes)\r\n\r\n\r\nlm2_jags <- function(){\r\n  # Likelihood:\r\n  for (i in 1:N){\r\n    b_mass[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)\r\n    mu[i] <- alpha[1] + sex[i] * alpha[2] + \r\n      (beta[1] + beta[2] * sex[i]) * b_length[i]\r\n  }\r\n  # Priors:\r\n  for (i in 1:2){\r\n    alpha[i] ~ dnorm(0, 0.01) \r\n    beta[i] ~ dnorm(0, 0.01)  \r\n  }\r\n  sigma ~ dunif(0, 1000) \r\n  tau <- 1 / (sigma * sigma)\r\n}\r\n\r\n# ---------\r\ninit_values <- function(){\r\n  list(alpha = rnorm(2), beta = rnorm(2), sigma = runif(1))\r\n}\r\n\r\nparams <- c(\"alpha\", \"beta\", \"sigma\")\r\n\r\nfit_lm2 <- jags(data = jagsdata_s2, \r\n                inits = init_values, \r\n                parameters.to.save = params, \r\n                model.file = lm2_jags, n.chains = 3, \r\n                n.iter = 12000, n.burnin = 2000, n.thin = 10, DIC = F)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n#--------\r\nlm2_mcmc <- as.mcmc(fit_lm2)\r\nplot(lm2_mcmc)\r\n\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but low variance.\r\n\r\n$b_mass\r\n [1] 42.27916 44.07903 20.24780 38.44588 15.68528 50.69550 45.73416\r\n [8] 23.60080 45.99189 19.97139 49.92215 25.28078 46.52624 31.62210\r\n[15] 51.85007 55.23480 25.46678 31.44646 35.42236 43.30904 24.30209\r\n[22] 43.31223 42.01626 49.86663 52.84152 35.53153 34.91018 45.12845\r\n[29] 40.27240 28.48178 32.54891 49.61999 31.66550 33.24145 52.61563\r\n[36] 52.40599 53.08417 30.85106 31.00310 44.72716 33.26385 33.51012\r\n[43] 68.72893 29.78999 37.54107 29.75238 48.69391 58.20429 35.10892\r\n[50] 59.13747\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but large variance.\r\nThe density of sigma shifted significanly.\r\n\r\n$b_mass\r\n [1]  53.8990908  59.4426007  29.5231515  20.7303588  -6.3103379\r\n [6]  80.9496405  50.8925931  25.3696005  43.5739562  -3.9151883\r\n[11]  62.1620864  20.9379797  42.8711028  50.2890263  68.2855351\r\n[16]  83.0771246  15.9433063  44.4534275  63.2445684  21.0932663\r\n[21]   7.0862415  20.6774554  12.8319806  51.4662785  65.9056104\r\n[26]  59.5508392  55.8052067  25.0642743  77.2420405  15.1463068\r\n[31]  34.6591869  41.1748751  29.2184963  37.0053126  54.9988444\r\n[36]  51.9041366  55.2456198  21.1423585  20.9187584  11.5051759\r\n[41]  25.6171744  23.2571170 122.7667536   2.5476609  40.2861977\r\n[46]  -0.1201204  19.2851945  60.6983335  15.1761402  59.1010162\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2022-08-22-Bayesian/2022-08-22-Bayes_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2024-08-13-network-analysis/",
    "title": "network",
    "description": "Keywords: Bayes Theorem, Bayesian application, Bayes interpretation.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [],
    "contents": "\r\n\r\n\r\n#install.packages(\"igraph\")\r\n#install.packages(\"igraphdata\")\r\nlibrary(igraph)\r\n#library(igraphdata)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(tidyverse)\r\n#install.packages(stringr)\r\nlibrary(ITNr)\r\nlibrary(igraphdata)\r\nlibrary(RColorBrewer)\r\n\r\n\r\n\r\n\r\n#dat3 <- read.csv(\"original_edgenode.csv\", header=T, as.is=T) \r\n\r\n\r\n\r\n\r\nedges <- read.csv(\"link.csv\")\r\nhead(edges)\r\n\r\n  from  to weight type\r\n1  s01 s02      1  lup\r\n2  s01 s10      1  lup\r\n3  s02 s10      1  lup\r\n4  s01 s03      1  rec\r\n5  s01 s04      2  rec\r\n6  s01 s05      5  rec\r\n\r\n\r\n\r\nnodes <- read.csv(\"node.csv\")\r\nhead(nodes)\r\n\r\n   id        agency agency.type type.label influence\r\n1 s01    CoPlanning           1      local        15\r\n2 s02 CoLegislative           1      local         5\r\n3 s03      CoBudget           1      local         5\r\n4 s04  CoPublicWork           1      local        10\r\n5 s05          CoEM           1      local        10\r\n6 s06       StateEM           2      state         5\r\n\r\n\r\n\r\nnrow(nodes); length(unique(nodes$id))\r\n\r\n[1] 18\r\n[1] 18\r\n\r\n\r\n\r\nnrow(edges); nrow(unique(edges[,c(\"from\", \"to\")]))\r\n\r\n[1] 136\r\n[1] 126\r\n\r\n\r\n\r\n#We will collapse all links of the same type between the same two nodes by summing their weights, using aggregate() by “from”, “to”, & “type”\r\nlinks <- aggregate(edges[,3], edges[,-3], sum)\r\nlinks <- links[order(links$from, links$to),]\r\ncolnames(links)[4] <- \"weight\"\r\nrownames(links) <- NULL\r\nlinks\r\n\r\n    from  to type weight\r\n1    s01 s02  lup      1\r\n2    s01 s03  rec      1\r\n3    s01 s04  mit      1\r\n4    s01 s04  rec      2\r\n5    s01 s05  mit      1\r\n6    s01 s05  rec      5\r\n7    s01 s06  mit      1\r\n8    s01 s06  res      1\r\n9    s01 s07  mit      1\r\n10   s01 s07  res      1\r\n11   s01 s08  mit      5\r\n12   s01 s09  mit      1\r\n13   s01 s10  lup      1\r\n14   s01 s10  mit      1\r\n15   s01 s12  mit      1\r\n16   s01 s13  mit      1\r\n17   s01 s14  mit      1\r\n18   s01 s14  res      1\r\n19   s01 s15  res      1\r\n20   s01 s16  res      1\r\n21   s01 s17  res      1\r\n22   s01 s18  res      1\r\n23   s01 s19  res      1\r\n24   s02 s10  lup      1\r\n25   s03 s04  rec      1\r\n26   s03 s05  rec      1\r\n27   s04 s05  mit      1\r\n28   s04 s05  rec      2\r\n29   s04 s06  mit      1\r\n30   s04 s07  mit      1\r\n31   s04 s08  mit      1\r\n32   s04 s09  mit      1\r\n33   s04 s10  mit      1\r\n34   s04 s12  mit      1\r\n35   s04 s13  mit      1\r\n36   s04 s14  mit      1\r\n37   s05 s06  mit      1\r\n38   s05 s07  mit      1\r\n39   s05 s08  mit      5\r\n40   s05 s09  mit      1\r\n41   s05 s10  mit      1\r\n42   s05 s12  mit      1\r\n43   s05 s13  mit      1\r\n44   s05 s14  mit      1\r\n45   s06 s01  res      1\r\n46   s06 s07  mit      1\r\n47   s06 s07  res      1\r\n48   s06 s08  mit      1\r\n49   s06 s09  mit      1\r\n50   s06 s10  mit      1\r\n51   s06 s12  mit      1\r\n52   s06 s13  mit      1\r\n53   s06 s14  mit      1\r\n54   s06 s14  res      1\r\n55   s06 s15  res      1\r\n56   s06 s16  res      1\r\n57   s06 s17  res      1\r\n58   s06 s18  res      1\r\n59   s06 s19  res      1\r\n60   s07 s01  res      1\r\n61   s07 s06  res      1\r\n62   s07 s08  mit      1\r\n63   s07 s09  mit      1\r\n64   s07 s10  mit      1\r\n65   s07 s12  mit      1\r\n66   s07 s13  mit      1\r\n67   s07 s14  mit      1\r\n68   s07 s14  res      1\r\n69   s07 s15  res      1\r\n70   s07 s16  res      1\r\n71   s07 s17  res      1\r\n72   s07 s18  res      1\r\n73   s07 s19  res      1\r\n74   s08 s09  mit      1\r\n75   s08 s10  mit      1\r\n76   s08 s12  mit      1\r\n77   s08 s13  mit      1\r\n78   s08 s14  mit      1\r\n79   s09 s10  mit      1\r\n80   s09 s12  mit      1\r\n81   s09 s13  mit      1\r\n82   s09 s14  mit      1\r\n83   s10 s12  mit      1\r\n84   s10 s13  mit      1\r\n85   s10 s14  mit      1\r\n86   s12 s13  mit      1\r\n87   s12 s14  mit      1\r\n88   s13 s14  mit      1\r\n89   s14 s01  res      1\r\n90   s14 s06  res      1\r\n91   s14 s07  res      1\r\n92   s14 s15  res      1\r\n93   s14 s16  res      1\r\n94   s14 s17  res      1\r\n95   s14 s18  res      1\r\n96   s14 s19  res      1\r\n97   s15 s01  res      1\r\n98   s15 s06  res      1\r\n99   s15 s07  res      1\r\n100  s15 s14  res      1\r\n101  s15 s16  res      1\r\n102  s15 s17  res      1\r\n103  s15 s18  res      1\r\n104  s15 s19  res      1\r\n105  s16 s01  res      1\r\n106  s16 s06  res      1\r\n107  s16 s07  res      1\r\n108  s16 s14  res      1\r\n109  s16 s15  res      1\r\n110  s16 s17  res      1\r\n111  s16 s18  res      1\r\n112  s16 s19  res      1\r\n113  s17 s01  res      1\r\n114  s17 s06  res      1\r\n115  s17 s07  res      1\r\n116  s17 s14  res      1\r\n117  s17 s15  res      1\r\n118  s17 s16  res      1\r\n119  s17 s18  res      1\r\n120  s17 s19  res      1\r\n121  s18 s01  res      1\r\n122  s18 s06  res      1\r\n123  s18 s07  res      1\r\n124  s18 s14  res      1\r\n125  s18 s15  res      1\r\n126  s18 s16  res      1\r\n127  s18 s17  res      1\r\n128  s18 s19  res      1\r\n129  s19 s01  res      1\r\n130  s19 s06  res      1\r\n131  s19 s07  res      1\r\n132  s19 s14  res      1\r\n133  s19 s15  res      1\r\n134  s19 s16  res      1\r\n135  s19 s17  res      1\r\n136  s19 s18  res      1\r\n\r\n\r\n\r\nnet <- graph_from_data_frame(d=links, vertices=nodes, directed=F)\r\nnet\r\n\r\nIGRAPH a837394 UNW- 18 136 -- \r\n+ attr: name (v/c), agency (v/c), agency.type (v/n),\r\n| type.label (v/c), influence (v/n), type (e/c), weight (e/n)\r\n+ edges from a837394 (vertex names):\r\n [1] s01--s02 s01--s03 s01--s04 s01--s04 s01--s05 s01--s05 s01--s06\r\n [8] s01--s06 s01--s07 s01--s07 s01--s08 s01--s09 s01--s10 s01--s10\r\n[15] s01--s12 s01--s13 s01--s14 s01--s14 s01--s15 s01--s16 s01--s17\r\n[22] s01--s18 s01--s19 s02--s10 s03--s04 s03--s05 s04--s05 s04--s05\r\n[29] s04--s06 s04--s07 s04--s08 s04--s09 s04--s10 s04--s12 s04--s13\r\n[36] s04--s14 s05--s06 s05--s07 s05--s08 s05--s09 s05--s10 s05--s12\r\n[43] s05--s13 s05--s14 s01--s06 s06--s07 s06--s07 s06--s08 s06--s09\r\n+ ... omitted several edges\r\n\r\n\r\n\r\n#find any particular nodes and edges by attribute\r\nE(net)$type\r\n\r\n  [1] \"lup\" \"rec\" \"mit\" \"rec\" \"mit\" \"rec\" \"mit\" \"res\" \"mit\" \"res\"\r\n [11] \"mit\" \"mit\" \"lup\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\"\r\n [21] \"res\" \"res\" \"res\" \"lup\" \"rec\" \"rec\" \"mit\" \"rec\" \"mit\" \"mit\"\r\n [31] \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\"\r\n [41] \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"mit\" \"res\" \"mit\" \"mit\" \"mit\"\r\n [51] \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n [61] \"res\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\" \"res\"\r\n [71] \"res\" \"res\" \"res\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\"\r\n [81] \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"mit\" \"res\" \"res\"\r\n [91] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[101] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[111] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[121] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n[131] \"res\" \"res\" \"res\" \"res\" \"res\" \"res\"\r\n\r\nV(net)$agency #vertex attribute \"agency name\"\r\n\r\n [1] \"CoPlanning\"    \"CoLegislative\" \"CoBudget\"      \"CoPublicWork\" \r\n [5] \"CoEM\"          \"StateEM\"       \"CoStormwater\"  \"CoExecutive\"  \r\n [9] \"CoInfo\"        \"CoBuilding\"    \"Insurance\"     \"DisasterSpec\" \r\n[13] \"HigherEdu\"     \"NOAA\"          \"USACE\"         \"StateResource\"\r\n[17] \"DisasterSpec2\" \"Consultant1\"  \r\n\r\n#find specific nodes \r\nV(net)[agency == \"CoPlanning\"]\r\n\r\n+ 1/18 vertex, named, from a837394:\r\n[1] s01\r\n\r\n#specific planning type\r\nE(net)[type ==\"lup\"]\r\n\r\n+ 3/136 edges from a837394 (vertex names):\r\n[1] s01--s02 s01--s10 s02--s10\r\n\r\n\r\n\r\n# Get data frames to describe nodes and edges \r\n#as_data_frame(net, what = \"edges\")\r\n\r\n\r\n\r\n\r\nplot(net, edge.color=\"grey\",\r\nvertex.color=\"orange\", vertex.frame.color=\"#ffffff\",  vertex.label.cex=.8,\r\nvertex.label=V(net)$agency, vertex.label.color=\"black\")\r\n\r\n\r\n\r\n\r\n\r\n# This plot is to visualize actors based on type (e.g., county, state, fed, private, nonprofit, private)\r\n# set attributes is to add them to the igraph object\r\n# Generate colors based on agency type:\r\ncolrs <- c(\"antiquewhite2\", \"grey\", \"gold\", \"coral\",\"aquamarine\")\r\n\r\nV(net)$color <- colrs[V(net)$agency.type]\r\nE(net)$color <- as.factor(links$type)\r\n# Compute node degrees (#links) and use that to set node size:\r\ndeg <- degree(net, mode=\"all\")\r\nV(net)$size <- deg*3\r\n\r\n# use the influence weight for nodes:\r\nV(net)$size <- V(net)$influence*2.2\r\n\r\n# Setting them to NA will render no labels:\r\nV(net)$label <- NA\r\n\r\n# Set edge width based on weight:\r\nE(net)$width <- E(net)$weight\r\n#change arrow size and edge color:\r\n#E(net)$arrow.size <- .2\r\nE(net)$edge.color <- \"azure4\"\r\nE(net)$width <- 1+E(net)$weight*.8\r\nplot(net, vertex.frame.color=\"#ffffff\",  vertex.label.cex=.6, \r\n     vertex.label=V(net)$agency, vertex.label.color=\"black\",\r\n     layout=layout.fruchterman.reingold)\r\n   #  layout=layout.fruchterman.reingold)\r\n\r\nlegend(\"right\", c(\"Local Agency\",\"State Agency\", \"Federal Agency\", \"Private Sector\", \"Nonprofit\"), pch=21,\r\n       col=\"#777777\", pt.bg=colrs, pt.cex=2, cex=.8, bty=\"n\", ncol=1)\r\n\r\n\r\n\r\n\r\n\r\ndeg_1 <- degree(net)\r\ndeg_1\r\n\r\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s12 s13 s14 s15 s16 s17 s18 \r\n 31   2   3  13  13  26  26  10  10  12  10  10  26  16  16  16  16 \r\ns19 \r\n 16 \r\n\r\nclo_1 <- closeness(net, vids = V(net), normalized = F)\r\nclo_1\r\n\r\n       s01        s02        s03        s04        s05        s06 \r\n0.05555556 0.03125000 0.03225806 0.04347826 0.04166667 0.05263158 \r\n       s07        s08        s09        s10        s12        s13 \r\n0.05263158 0.03846154 0.04166667 0.04347826 0.04166667 0.04166667 \r\n       s14        s15        s16        s17        s18        s19 \r\n0.05263158 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 \r\n\r\nbet_1 <- betweenness(net, normalized = F)\r\nbet_1\r\n\r\n       s01        s02        s03        s04        s05        s06 \r\n27.3000000  0.0000000  0.0000000  3.0416667  1.8500000  9.2416667 \r\n       s07        s08        s09        s10        s12        s13 \r\n 9.2416667  0.0000000  0.1916667  4.5083333  0.1916667  0.1916667 \r\n       s14        s15        s16        s17        s18        s19 \r\n 9.2416667  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2024-08-13-network-analysis/prac3_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2024-08-30T01:04:04-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2016-04-20-time-series/",
    "title": "Time Series Analysis with Examples",
    "description": "This is a project from undergraduate studies, I post it for getting familiar with some basic concepts of time series analyses.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\r\nMotivation\r\nMovies have been a source of entertainment for people over years.\r\nSummertime superhero blockbusters typically bring excitement to an\r\notherwise calm time. Theaters can rake in huge sums of money on these\r\ntypes of movies. In particular the movie Iron Man 3 released on May 3r\r\n2013 provides a great example of this phenomenon. The box of ticket\r\nsales of this movie have been analyzed to ft the best time series plot\r\npossible to attempt to approximate future movie ticket sales for movies\r\nof this type.\r\nMethodology\r\nTo complete the analysis data on ticket sales were gathered from https://www.the-numbers.com/ for Iron Man 3. Daily and\r\nweekly per-theater ticket sales were collected and fitted to several\r\ntime series models. The models included moving average single\r\nexponential Holt-t Winters simple linear regression and multiple linear\r\nregression. Each model was judged based on three accuracy measures: MAD\r\nMAPE and MSD. The best model for each type of data (daily and weekly)\r\nwas selected and recommended to be used in the prediction of future\r\nmovie releases.\r\nResults and Analysis\r\nFor the Time Series Plot of Weekly Per Theater Ticket Sales (Figure\r\n1)r there is a decreasing trend. The value of per-theater ticket sales\r\ndecreases until a certain point before leveling out about 5 weeks after\r\nrelease. There is no cyclical or seasonal effect that can be found from\r\nthe time series plot and the plot looks fairly smooth. For the week\r\n2013/5/24 and the week 2013/08/16r there were slight increasing in both\r\nweeks the reason of increasing in May could be the end of semester and\r\nstudents throwing parts after finals. In August good weather and nearing\r\nthe end of summer breaks may play a factor.\r\n The Time Series Plot of Daily\r\nTicket Sales (Figure 2), overall there is a decreasing trend starting\r\nfrom May and almost leveled off in June. There is a seasonal variation\r\ncan be found for daily because people are more likely to go to movies on\r\nweekends than weekdays. The seasonal variation shows a 7-day repeating\r\npattern with 3-day peak, representing Friday, Saturday and Sunday, which\r\nmore people prefer to go to movies on.\r\nFigure 2Weekly\r\nFor the weekly ticket sales, analysis was conducted for the moving\r\naverage, exponential smoothing, Holt-Winters, and simple linear\r\nregression models with accuracy measures shown in Table 1. Based on\r\nthese accuracy measures, the moving average was by far the best model,\r\nas seen in Figure 3. All three accuracy measures were the lowest of any\r\nmodel. The moving average model was fit with an MA length of 4 and the\r\nmoving averages were centered.\r\nThe exponential model was fit using an alpha of 0.8. Initial attempts\r\nto optimize ARIMA to find an ideal alpha did not work, as Minitab threw\r\nan error code. After adjusting the alpha value through several\r\niterations, a value of 0.8 was selected as the best. With this value,\r\nthe model was still objectively worse than the moving average in all\r\naccuracy measures.\r\nFigureFigure 3The same process was used (multiple iterations to find the best\r\nvalues) for the Holt-Winters method with a final model using an alpha of\r\n0.8, a gamma of 0.3, and a delta of 0. The delta was selected to be 0\r\nbecause the weekly ticket sales showed no seasonal trend. This model was\r\nagain worse than the moving average.\r\nFinally, a simple linear regression was constructed. The model fit\r\nthe data to the equation TicketSales = 21943 – 1715*(days after\r\nrelease). This model was calculated to have worse accuracy measures than\r\nthe moving average. A multiple regression, using seasons as categorical\r\nvariables, was not conducted as there was no seasonal trend for the\r\nweekly dataset. As such, trying to force a fit on that data set would\r\nnot be effective.\r\nOverall, the moving average model was best for the weekly,\r\nper-theater ticket sales for Iron Man 3. This model had the best\r\naccuracy measures and makes sense. The sales values level off very\r\nquickly and stay effectively horizontal for most of the plot, which the\r\nmoving average model handles well. Additionally, none of the models\r\nhandled the first few data points very well, and the moving average had\r\nthe benefit of not starting to fit points until the third week, due to\r\nthe nature of the analysis.\r\nAs a secondary option, the Holt-Winters model also fit the data\r\nreasonably well (Figure 4). While there was a large overprediction for\r\nthe first week, and some underprediction for the following several\r\nweeks, the model did a good job tracking the trend and pattern of the\r\nticket sales. The sort of decay pattern is matched well, despite not\r\nhaving any seasonal effect (which the Holt-Winters tries to model).\r\nAdditionally, this model had the second best MAD, and MSD.\r\nFigure 4Daily\r\nFor daily ticket sales, the analysis found slightly different\r\nresults. As mentioned above, the daily ticket sales showed a definite\r\nseasonal pattern, and this makes sense. Weekends will have much higher\r\nbox office sales than weekdays. As such, a model that could take\r\nseasonality into account would likely be much better.\r\nThis tended to show true as the Holt-Winters method had the best\r\naccuracy measures in two out of three cases (the MAPE and MAD were best\r\nfor Holt-Winters). The Holt-Winters model, as seen in Figure 5, was able\r\nto handle the seasonality, which the moving average and single\r\nexponential could not do. They both had accuracy measures much higher\r\nthan the Holt-Winters method.\r\nFigureFigure 5Regression models were also fit with a single regression model having\r\nan equation of TicketSales = 4821 - 99.1(days since release). This model\r\nfit very poorly, however, due to the seasonality of the data a multiple\r\nregression model was also fit. Days since release was used as a\r\ncontinuous predictor with days of the week used as categorical\r\npredictors. The best equation was found by including a second-order term\r\nfor days since release and using an interaction between days since\r\nrelease and days of the week. The actual equation was found to be the\r\nfollowing, based on day of the week:\r\nSunday Daily Ticket \\[Sales = 9544 - 413.8\r\n(Days Since Release) + 4.530 (Days Since Release)^2 \\]\r\nMonday Daily Ticket \\[Sales = 5067 - 324.1\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nTuesday Daily Ticket \\[Sales = 5216 -\r\n331.3 (Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nWednesday Daily Ticket \\[Sales = 4474 - 304.0\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\] Thursday\r\nDaily Ticket \\[Sales = 4671 - 310.7 (Days\r\nSince Release) + 4.530 (Days Since Release)^2\\] Friday Daily\r\nTicket \\[Sales = 10803 - 447.1 (Days Since\r\nRelease) + 4.530 (Days Since Release)^2\\] Saturday Daily Ticket\r\n\\[Sales = 12051 - 464.4 (Days Since Release)\r\n+ 4.530 (Days Since Release)^2\\]\r\nThis regression equation had accuracy measures calculated manually to\r\nbe 143; 977; and 1,825,432 for the MAPE, MAD, and MSD, respectively.\r\nThese measures were objectively worse than those for other models, so\r\nthe multiple regression model was not used.\r\nThe moving average was selected as an alternate model (Figure 6), as\r\nits accuracy measures were generally good. It had the best MSD, second\r\nbest MAD, and third MAPE of all the models. While this model did not do\r\ngreat in the first week, it steadied out and modeled the relative\r\nflatness of later ticket sales pretty well.\r\nFigure 6Conclusion\r\nBy using the data collected from http://www.thenumbers.com of the movie The Iron Man 3,\r\ndifferent times series models were generated including moving average,\r\nsingle exponential smoothing, Holt-Winters, simple linear regression,\r\nand multiple linear regression. For the weekly ticket sales, based on\r\nthe analysis among all models, a moving average should be chosen because\r\nit had the best accuracy measures (MAPE, MAD, and MSD). For the daily\r\nticket sale, there is not just a trend, but a seasonal pattern within\r\nthe time series. So, the Holt-Winters method did a good job on\r\npredicting ticket sales. With this information, it is fair to assume\r\nthat in the prediction of future movie ticket sales, a moving average\r\nwould be best to determine weekly ticket sales and a Holt-Winters model\r\nwould be best for daily ticket sales. Unfortunately, neither model did a\r\ngreat job of predicting opening day/week sales, but the models steadied\r\nafter that. So, in conclusion the models mentioned above are useful in\r\npredicting ticket sales for summer blockbuster superhero movies, and may\r\nbe useful in predicting sales for all movies; however, the prediction is\r\nlimited to times beyond the opening week.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {}
  }
]
