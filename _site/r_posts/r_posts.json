[
  {
    "path": "r_posts/2022-08-22-Bayesian/",
    "title": "A Bayes Application and made up examples",
    "description": "Keywords: Bayes Theorem, Bayesian application, Bayes interpretation.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [],
    "contents": "\r\nThis post is my own practice and preparation of using/applying\r\nBayesian regression. Material and example codes are from John\r\nMyles White.\r\nThe first step is to create a set of data that fulfills the following\r\nrequirements: Gaussian distributed with equal variance and a meang which\r\nis a linear function of the predictors.\r\n\r\n    b_length    b_mass\r\n1 -2.6564554  5.712696\r\n2 -2.4404669  9.119517\r\n3 -1.7813084 17.362433\r\n4 -1.7631631  9.323737\r\n5 -1.3888607 18.636169\r\n6 -0.6399949 15.015008\r\n\r\nNow, before analyzing, convert to the list format. The list will have\r\nthree elements, body mass, body length, and a number that indicates the\r\nsample size.\r\nWrite JAGS model. Create the model with JAGS code, there are\r\ndifferences between R and JAGS code: JAGS use dnorm for distribution’s\r\nprecision (1/variance). Tilde (~) signifies that object on the left is\r\nrandom variable distributed according to the distribution on the right.\r\nAlpha and Beta are intercepts of linear relationships.\r\n\r\n\r\n\r\nThe initial parameter values for the MCMC. The parameters are chosen\r\nwhose posterior distributions will be reported and runs the model in\r\nJAGS.\r\nThree MCMC chanins with 12000 iterations each, and discards the first\r\n2000 values, because these first 2000 values highly depend on initial\r\nvalues.\r\nn.thin means only every 10th iteration is saved and the rest\r\ndiscarded.\r\nModel output. The 2.5% and 97.5% quantile range are the 95% credible\r\ninterval for each parameter. For the 95% credible interval, there is a\r\n95% probability that the true parameter lies within this range.\r\nThe last two columns are convergence diagnostics. n.eff is a number\r\nsmaller than or equal to the number of samples saved from the chains\r\n(3*(12000-2000)/10). The higher autocorrelation in the saved samples\r\nindicate a smaller effective sample size. Rhat indicates how well the\r\nthree Markov chains are mixed. Ideal Rhat is close to 1. Large Rhat\r\nvalues mean that chains are not mixed properly and posterior estimates\r\ncannot be trusted.\r\n\r\n\r\nfit_lm1\r\n\r\nInference for Bugs model at \"/var/folders/xj/c74jv4d928z1rmtv4_vchj740000gn/T//RtmpKVxs4V/model158f583cb405.txt\", fit using jags,\r\n 3 chains, each with 12000 iterations (first 2000 discarded), n.thin = 10\r\n n.sims = 3000 iterations saved\r\n      mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\r\nalpha  29.079   1.038 27.063 28.408 29.101 29.761 31.086 1.002  1800\r\nbeta    9.736   0.845  8.044  9.199  9.744 10.294 11.342 1.001  3000\r\nsigma   5.586   0.812  4.275  5.015  5.485  6.074  7.371 1.001  3000\r\n\r\nFor each parameter, n.eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\r\n\r\nThe traceplot is used to visualize how well the chains have mixed. In\r\nthis case, chains are nicely mixed\r\n\r\n\r\n\r\nOther ways to demonstrate posterior distributions of parameters\r\ngraphically.\r\n\r\n\r\n\r\nVisualize in MCMC format\r\n\r\n\r\n\r\nIf we change parameters and plot the mcmc visuals, we want to see the\r\ndifferences when altering.\r\nCreate\r\na Catogorical variable (sex) and adding to the jags model\r\n\r\n$b_mass\r\n [1] 19.309795 21.757625  8.176761 18.101959  6.869465 30.544474\r\n [7] 26.567582 16.759391 28.439164 15.914424 32.568771 21.647253\r\n[13] 29.606252 28.577468 35.155978 38.940458 23.314439 29.641170\r\n[19] 33.889166 27.456285 22.908149 27.540421 26.500509 34.466663\r\n[25] 37.523150 35.058236 34.596614 30.236815 41.302396 30.297417\r\n[31] 34.570252 35.918447 33.942750 35.541939 39.130491 39.304575\r\n[37] 39.992016 34.129303 34.527288 32.001647 38.439367 39.583496\r\n[43] 57.643543 36.390552 44.395866 36.972887 38.228475 48.889866\r\n[49] 45.201041 50.997405\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\n\r\nTest\r\nof increasing the range of data (by increasing the standard deviation 10\r\ntimes)\r\n\r\n\r\nlm2_jags <- function(){\r\n  # Likelihood:\r\n  for (i in 1:N){\r\n    b_mass[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)\r\n    mu[i] <- alpha[1] + sex[i] * alpha[2] + \r\n      (beta[1] + beta[2] * sex[i]) * b_length[i]\r\n  }\r\n  # Priors:\r\n  for (i in 1:2){\r\n    alpha[i] ~ dnorm(0, 0.01) \r\n    beta[i] ~ dnorm(0, 0.01)  \r\n  }\r\n  sigma ~ dunif(0, 1000) \r\n  tau <- 1 / (sigma * sigma)\r\n}\r\n\r\n# ---------\r\ninit_values <- function(){\r\n  list(alpha = rnorm(2), beta = rnorm(2), sigma = runif(1))\r\n}\r\n\r\nparams <- c(\"alpha\", \"beta\", \"sigma\")\r\n\r\nfit_lm2 <- jags(data = jagsdata_s2, \r\n                inits = init_values, \r\n                parameters.to.save = params, \r\n                model.file = lm2_jags, n.chains = 3, \r\n                n.iter = 12000, n.burnin = 2000, n.thin = 10, DIC = F)\r\n\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n#--------\r\nlm2_mcmc <- as.mcmc(fit_lm2)\r\nplot(lm2_mcmc)\r\n\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but low variance.\r\n\r\n$b_mass\r\n [1] 42.27916 44.07903 20.24780 38.44588 15.68528 50.69550 45.73416\r\n [8] 23.60080 45.99189 19.97139 49.92215 25.28078 46.52624 31.62210\r\n[15] 51.85007 55.23480 25.46678 31.44646 35.42236 43.30904 24.30209\r\n[22] 43.31223 42.01626 49.86663 52.84152 35.53153 34.91018 45.12845\r\n[29] 40.27240 28.48178 32.54891 49.61999 31.66550 33.24145 52.61563\r\n[36] 52.40599 53.08417 30.85106 31.00310 44.72716 33.26385 33.51012\r\n[43] 68.72893 29.78999 37.54107 29.75238 48.69391 58.20429 35.10892\r\n[50] 59.13747\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\nTest of\r\nJAGS model with wrong mean but large variance.\r\nThe density of sigma shifted significanly.\r\n\r\n$b_mass\r\n [1]  53.8990908  59.4426007  29.5231515  20.7303588  -6.3103379\r\n [6]  80.9496405  50.8925931  25.3696005  43.5739562  -3.9151883\r\n[11]  62.1620864  20.9379797  42.8711028  50.2890263  68.2855351\r\n[16]  83.0771246  15.9433063  44.4534275  63.2445684  21.0932663\r\n[21]   7.0862415  20.6774554  12.8319806  51.4662785  65.9056104\r\n[26]  59.5508392  55.8052067  25.0642743  77.2420405  15.1463068\r\n[31]  34.6591869  41.1748751  29.2184963  37.0053126  54.9988444\r\n[36]  51.9041366  55.2456198  21.1423585  20.9187584  11.5051759\r\n[41]  25.6171744  23.2571170 122.7667536   2.5476609  40.2861977\r\n[46]  -0.1201204  19.2851945  60.6983335  15.1761402  59.1010162\r\n\r\n$b_length\r\n [1] -2.65645542 -2.44046693 -2.41420765 -1.78130843 -1.76316309\r\n [6] -1.71700868 -1.38886070 -1.36828104 -0.85090759 -0.81139318\r\n[11] -0.78445901 -0.72670483 -0.63999488 -0.60892638 -0.56469817\r\n[16] -0.43144620 -0.43046913 -0.36105730 -0.30663859 -0.28425292\r\n[21] -0.27878877 -0.25726938 -0.17191736 -0.13332134 -0.10612452\r\n[26] -0.09465904 -0.06271410  0.03612261  0.20599860  0.36312841\r\n[31]  0.40426832  0.43281803  0.45545012  0.46009735  0.50495512\r\n[36]  0.63286260  0.63595040  0.65564788  0.70483734  0.75816324\r\n[41]  1.03510352  1.21467470  1.30486965  1.32011335  1.37095845\r\n[46]  1.44410126  1.51152200  1.89519346  2.01842371  2.28664539\r\n\r\n$sex\r\n [1] 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0\r\n[34] 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1\r\n\r\n$N\r\n[1] 50\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 50\r\n   Unobserved stochastic nodes: 5\r\n   Total graph size: 268\r\n\r\nInitializing model\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2022-08-22-Bayesian/2022-08-22-Bayes_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2018-05-01-Iowa/",
    "title": "Iowa Pop Characteristics",
    "description": "A descriptive statistics for population in Iowa.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2018-05-01",
    "categories": [],
    "contents": "\r\n\r\nIntroduction\r\nThe goal of this study is to look at:\r\nWhat is the relationships between the population and other\r\nvariables, such as education attainment, poverty, the mean of income,\r\nand the median house value.\r\nVisually display the distribution of median house values across the\r\nstate, and three frequency distributions based on the population of a\r\ncounty. All counties are evenly divided into three tiers:\r\nTier 1: population size from 3,000 to 11,000 (33 counties)\r\nTier 2: population size form 11,000 to 20,000 (33 counties)\r\nTier 3: population size form 20,000 to 48,200 (33 counties)\r\n\r\nCorrelations between:\r\nNumber of bechelor received and number of poverty.\r\nMedian house value and the mean of income.\r\nNumber of employed and the mean of income.\r\nNumber of employed and the number under poverty.\r\n\r\nT test.\r\nAll data are collected through the U.S. Census at https://www.census.gov/.\r\nMethods\r\nload Packages\r\nIn order to accomplish these following tasks:\r\nData manipulation\r\nCorrelation\r\nHistogram Visualization\r\n2-sample t test\r\nI use the these following packages in R:\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(magrittr)\r\nlibrary(ggplot2)\r\nlibrary(ggpubr)\r\nlibrary(psych)\r\nlibrary(Hmisc)\r\nlibrary(devtools)\r\n\r\n\r\nImport Data\r\nI used a .csv file to store all collected data from the U.S. Census.\r\nMy target are 99 counties of the state of Iowa.\r\n\r\n\r\n\r\nCheck Variables\r\nBy using head, we are able to check all variables\r\ncollected for future data analysis. From U.S. Census Data, I collected\r\nthe following data as my variables: data2010 population, 2017\r\npopulation, number of bachelor obtained for over 25 year-old, population\r\ncount that is below poverty, mean of income, and the median house\r\nvalue\r\n\r\n              Id             Geography pop.2010 pop.2017 by.pop\r\n1 0500000US19003    Adams County, Iowa     4029     3686      1\r\n2 0500000US19159 Ringgold County, Iowa     5131     5034      1\r\n3 0500000US19009  Audubon County, Iowa     6119     5578      1\r\n4 0500000US19143  Osceola County, Iowa     6462     6045      1\r\n5 0500000US19173   Taylor County, Iowa     6317     6178      1\r\n6 0500000US19185    Wayne County, Iowa     6403     6476      1\r\n  Over25.Bach below.poverty median.house mean.income employed\r\n1         357           418        84900       76251     5935\r\n2         521           518        79600       77205     3116\r\n3         414           614        68800       79727    11189\r\n4         595           689        89500       73473    10108\r\n5         446           668        76100       74703     4783\r\n6         416          1001        74700       65938    20110\r\n\r\nUnivariate Statistics\r\nBy running summary, we are able to learn some basic\r\nstatistics of our dataset. These statistics are range, mean and\r\nmedian.\r\n\r\n      Id             Geography            pop.2010     \r\n Length:99          Length:99          Min.   :  4029  \r\n Class :character   Class :character   1st Qu.: 10430  \r\n Mode  :character   Mode  :character   Median : 15679  \r\n                                       Mean   : 30771  \r\n                                       3rd Qu.: 24631  \r\n                                       Max.   :430640  \r\n    pop.2017          by.pop   Over25.Bach      below.poverty  \r\n Min.   :  3686   Min.   :1   Min.   :  357.0   Min.   :  418  \r\n 1st Qu.:  9977   1st Qu.:1   1st Qu.:  944.5   1st Qu.: 1057  \r\n Median : 15224   Median :2   Median : 1544.0   Median : 1570  \r\n Mean   : 31775   Mean   :2   Mean   : 3908.7   Mean   : 3650  \r\n 3rd Qu.: 25066   3rd Qu.:3   3rd Qu.: 2718.0   3rd Qu.: 2527  \r\n Max.   :481830   Max.   :3   Max.   :75457.0   Max.   :53570  \r\n  median.house     mean.income        employed     \r\n Min.   : 68800   Min.   : 60582   Min.   :  3116  \r\n 1st Qu.: 91050   1st Qu.: 75504   1st Qu.:  8114  \r\n Median :107700   Median : 79876   Median : 12194  \r\n Mean   :113776   Mean   : 81653   Mean   : 24844  \r\n 3rd Qu.:130600   3rd Qu.: 85658   3rd Qu.: 19728  \r\n Max.   :218000   Max.   :129274   Max.   :354927  \r\n\r\nHistograms and\r\nDensity/Frequency Distributions\r\n\r\n\r\ny <- my_data$median.house\r\nhist(y,main = \"Distribution of Median House Value\", xlab = \"Value Range\", \r\n     xlim = c(0, 220000), col = \"blue\", border =\"black\", prob=TRUE)\r\nlines(density(y, adjust = 2), col = \"red\")\r\n\r\n\r\n\r\nRecall: based on the population size, we devide 99 counties into\r\nthree tiers of counties. 1. Population between 3,000 and 11,000: 33\r\ncounties - pop1 - Tier 1 2. Population between 11,000 and 20,000: 33\r\ncounties - pop2 - Tier 2 3. Population between 20,000 and 482,000: 33\r\ncounties - pop3 - Tier 3\r\n\r\n\r\nx <- my_data$mean.income\r\nhist(x,main = \"Distribution of Mean of Income\", xlab = \"Income Range\", \r\n     col = \"blue\", border =\"black\", prob = T )\r\nlines(density(x, adjust = 2), col = \"red\")\r\n\r\n\r\n\r\nBased on the population size, I devide 99 counties into three tiers\r\nof counties.\r\nPopulation between 3600 and 11000: 33 counties - pop1 - Tier 1\r\nPopulation between 11000 and 20000: 33 counties - pop2 - Tier 2\r\nPopulation between 20000 and 482000: 33 counties - pop3 - Tier\r\n3\r\n\r\n\r\npop1 <- filter(my_data, by.pop == 1)\r\ny1 <- pop1$mean.income\r\nhist(y1,main = \"Income Distribution of Counties in the First Tier\", xlab = \"Income Range\",\r\n     xlim = c(60000, 150000), col = \"blue\", border =\"black\", prob=TRUE)\r\nlines(density(y1, adjust = 2), col = \"red\")\r\n\r\n\r\n\r\n\r\n[1] 76251 77205 79727 73473 74703 65938\r\n\r\n\r\n              Id             Geography pop.2010 pop.2017 by.pop\r\n1 0500000US19003    Adams County, Iowa     4029     3686      1\r\n2 0500000US19159 Ringgold County, Iowa     5131     5034      1\r\n3 0500000US19009  Audubon County, Iowa     6119     5578      1\r\n4 0500000US19143  Osceola County, Iowa     6462     6045      1\r\n5 0500000US19173   Taylor County, Iowa     6317     6178      1\r\n6 0500000US19185    Wayne County, Iowa     6403     6476      1\r\n  Over25.Bach below.poverty median.house mean.income employed\r\n1         357           418        84900       76251     5935\r\n2         521           518        79600       77205     3116\r\n3         414           614        68800       79727    11189\r\n4         595           689        89500       73473    10108\r\n5         446           668        76100       74703     4783\r\n6         416          1001        74700       65938    20110\r\n\r\n\r\n\r\npop2 <- filter(my_data, by.pop == 2)\r\ny2 <- pop2$mean.income\r\nhist(y2,main = \"Income Distribution of Counties in the Second Tier\", xlab = \"Income Range\", \r\n     xlim = c(60000, 150000), col = \"blue\", border =\"black\", prob=TRUE)\r\nlines(density(y2, adjust = 2), col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\npop3 <- filter(my_data, by.pop == 3)\r\ny3 <- pop3$mean.income\r\nhist(y3,main = \"Income Distribution of Counties in the Third Tier\", xlab = \"Income Range\",\r\n     xlim = c(60000, 150000), col = \"blue\", border =\"black\", prob=TRUE)\r\nlines(density(y3, adjust = 2), col = \"red\")\r\n\r\n\r\n\r\nConlusion\r\nBased on the frequency distribution, as the size of population of a\r\ncounty increaces, the average income tends to increase.\r\nCorrelation\r\nbetween number of bachelor over 25 in age and number under poverty\r\nNormally, we would expact the correlation between number of bechalor\r\ndegrees obtained and the number of people under poverty. However, the\r\ntruth can be: the more bechalors a county has, the bigger population it\r\nmay have. Therefore, it is more likely to have more people under\r\npoverty.\r\n\r\n\r\ncorrelation1 <- cor(my_data$Over25.Bach, my_data$below.poverty, \r\n                   method = c(\"pearson\"))\r\ncorrelation1\r\n\r\n[1] 0.9541034\r\n\r\nCorrelation\r\nbetween median house value and mean of income.\r\nThere is a strong positive correlation between median house value and\r\nthe average income.\r\n\r\n[1] 0.8414412\r\n\r\nCorrelation\r\nbetween number of employed and mean of income.\r\nIt is very likely that there is no correlation between number of\r\nemployed and the mean of income.\r\n\r\n[1] 0.04853983\r\n\r\nCorrelation\r\nbetween number of employed and number under poverty.\r\nIt is very likely that there is no correlation between number of\r\nemployed and the number under poverty\r\n\r\n\r\ncorrelation4 <- cor(my_data$employed, my_data$below.poverty, \r\n                    method = c(\"pearson\"))\r\ncorrelation4\r\n\r\n[1] -0.004254535\r\n\r\nT- test\r\nNull: the means of income are the same.\r\nAlternative: they are not the same.\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  my_data$pop.2017 and my_data$mean.income\r\nt = -8.3951, df = 196, p-value = 9.212e-15\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -61594.95 -38160.87\r\nsample estimates:\r\nmean of x mean of y \r\n 31774.86  81652.77 \r\n\r\nThe p-value is smaller than 0.05. There is significant evidence to\r\nreject the null hypothesis. The means of income are not the same for\r\ncounties with different population sizes.\r\n\r\n\r\n\r\n",
    "preview": "r_posts/2018-05-01-Iowa/2018-05-01-Iowa_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "r_posts/2016-04-20-time-series/",
    "title": "Time Series Analysis with Examples",
    "description": "This is a project from undergraduate studies, I post it for getting familiar with some basic concepts of time series analyses.",
    "author": [
      {
        "name": "Yiwen Wu",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\r\nMotivation\r\nMovies have been a source of entertainment for people over years.\r\nSummertime superhero blockbusters typically bring excitement to an\r\notherwise calm time. Theaters can rake in huge sums of money on these\r\ntypes of movies. In particular the movie Iron Man 3 released on May 3r\r\n2013 provides a great example of this phenomenon. The box of ticket\r\nsales of this movie have been analyzed to ft the best time series plot\r\npossible to attempt to approximate future movie ticket sales for movies\r\nof this type.\r\nMethodology\r\nTo complete the analysis data on ticket sales were gathered from https://www.the-numbers.com/ for Iron Man 3. Daily and\r\nweekly per-theater ticket sales were collected and fitted to several\r\ntime series models. The models included moving average single\r\nexponential Holt-t Winters simple linear regression and multiple linear\r\nregression. Each model was judged based on three accuracy measures: MAD\r\nMAPE and MSD. The best model for each type of data (daily and weekly)\r\nwas selected and recommended to be used in the prediction of future\r\nmovie releases.\r\nResults and Analysis\r\nFor the Time Series Plot of Weekly Per Theater Ticket Sales (Figure\r\n1)r there is a decreasing trend. The value of per-theater ticket sales\r\ndecreases until a certain point before leveling out about 5 weeks after\r\nrelease. There is no cyclical or seasonal effect that can be found from\r\nthe time series plot and the plot looks fairly smooth. For the week\r\n2013/5/24 and the week 2013/08/16r there were slight increasing in both\r\nweeks the reason of increasing in May could be the end of semester and\r\nstudents throwing parts after finals. In August good weather and nearing\r\nthe end of summer breaks may play a factor.\r\n The Time Series Plot of Daily\r\nTicket Sales (Figure 2), overall there is a decreasing trend starting\r\nfrom May and almost leveled off in June. There is a seasonal variation\r\ncan be found for daily because people are more likely to go to movies on\r\nweekends than weekdays. The seasonal variation shows a 7-day repeating\r\npattern with 3-day peak, representing Friday, Saturday and Sunday, which\r\nmore people prefer to go to movies on.\r\nFigure 2Weekly\r\nFor the weekly ticket sales, analysis was conducted for the moving\r\naverage, exponential smoothing, Holt-Winters, and simple linear\r\nregression models with accuracy measures shown in Table 1. Based on\r\nthese accuracy measures, the moving average was by far the best model,\r\nas seen in Figure 3. All three accuracy measures were the lowest of any\r\nmodel. The moving average model was fit with an MA length of 4 and the\r\nmoving averages were centered.\r\nThe exponential model was fit using an alpha of 0.8. Initial attempts\r\nto optimize ARIMA to find an ideal alpha did not work, as Minitab threw\r\nan error code. After adjusting the alpha value through several\r\niterations, a value of 0.8 was selected as the best. With this value,\r\nthe model was still objectively worse than the moving average in all\r\naccuracy measures.\r\nFigureFigure 3The same process was used (multiple iterations to find the best\r\nvalues) for the Holt-Winters method with a final model using an alpha of\r\n0.8, a gamma of 0.3, and a delta of 0. The delta was selected to be 0\r\nbecause the weekly ticket sales showed no seasonal trend. This model was\r\nagain worse than the moving average.\r\nFinally, a simple linear regression was constructed. The model fit\r\nthe data to the equation TicketSales = 21943 – 1715*(days after\r\nrelease). This model was calculated to have worse accuracy measures than\r\nthe moving average. A multiple regression, using seasons as categorical\r\nvariables, was not conducted as there was no seasonal trend for the\r\nweekly dataset. As such, trying to force a fit on that data set would\r\nnot be effective.\r\nOverall, the moving average model was best for the weekly,\r\nper-theater ticket sales for Iron Man 3. This model had the best\r\naccuracy measures and makes sense. The sales values level off very\r\nquickly and stay effectively horizontal for most of the plot, which the\r\nmoving average model handles well. Additionally, none of the models\r\nhandled the first few data points very well, and the moving average had\r\nthe benefit of not starting to fit points until the third week, due to\r\nthe nature of the analysis.\r\nAs a secondary option, the Holt-Winters model also fit the data\r\nreasonably well (Figure 4). While there was a large overprediction for\r\nthe first week, and some underprediction for the following several\r\nweeks, the model did a good job tracking the trend and pattern of the\r\nticket sales. The sort of decay pattern is matched well, despite not\r\nhaving any seasonal effect (which the Holt-Winters tries to model).\r\nAdditionally, this model had the second best MAD, and MSD.\r\nFigure 4Daily\r\nFor daily ticket sales, the analysis found slightly different\r\nresults. As mentioned above, the daily ticket sales showed a definite\r\nseasonal pattern, and this makes sense. Weekends will have much higher\r\nbox office sales than weekdays. As such, a model that could take\r\nseasonality into account would likely be much better.\r\nThis tended to show true as the Holt-Winters method had the best\r\naccuracy measures in two out of three cases (the MAPE and MAD were best\r\nfor Holt-Winters). The Holt-Winters model, as seen in Figure 5, was able\r\nto handle the seasonality, which the moving average and single\r\nexponential could not do. They both had accuracy measures much higher\r\nthan the Holt-Winters method.\r\nFigureFigure 5Regression models were also fit with a single regression model having\r\nan equation of TicketSales = 4821 - 99.1(days since release). This model\r\nfit very poorly, however, due to the seasonality of the data a multiple\r\nregression model was also fit. Days since release was used as a\r\ncontinuous predictor with days of the week used as categorical\r\npredictors. The best equation was found by including a second-order term\r\nfor days since release and using an interaction between days since\r\nrelease and days of the week. The actual equation was found to be the\r\nfollowing, based on day of the week:\r\nSunday Daily Ticket \\[Sales = 9544 - 413.8\r\n(Days Since Release) + 4.530 (Days Since Release)^2 \\]\r\nMonday Daily Ticket \\[Sales = 5067 - 324.1\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nTuesday Daily Ticket \\[Sales = 5216 -\r\n331.3 (Days Since Release) + 4.530 (Days Since Release)^2\\]\r\nWednesday Daily Ticket \\[Sales = 4474 - 304.0\r\n(Days Since Release) + 4.530 (Days Since Release)^2\\] Thursday\r\nDaily Ticket \\[Sales = 4671 - 310.7 (Days\r\nSince Release) + 4.530 (Days Since Release)^2\\] Friday Daily\r\nTicket \\[Sales = 10803 - 447.1 (Days Since\r\nRelease) + 4.530 (Days Since Release)^2\\] Saturday Daily Ticket\r\n\\[Sales = 12051 - 464.4 (Days Since Release)\r\n+ 4.530 (Days Since Release)^2\\]\r\nThis regression equation had accuracy measures calculated manually to\r\nbe 143; 977; and 1,825,432 for the MAPE, MAD, and MSD, respectively.\r\nThese measures were objectively worse than those for other models, so\r\nthe multiple regression model was not used.\r\nThe moving average was selected as an alternate model (Figure 6), as\r\nits accuracy measures were generally good. It had the best MSD, second\r\nbest MAD, and third MAPE of all the models. While this model did not do\r\ngreat in the first week, it steadied out and modeled the relative\r\nflatness of later ticket sales pretty well.\r\nFigure 6Conclusion\r\nBy using the data collected from http://www.thenumbers.com of the movie The Iron Man 3,\r\ndifferent times series models were generated including moving average,\r\nsingle exponential smoothing, Holt-Winters, simple linear regression,\r\nand multiple linear regression. For the weekly ticket sales, based on\r\nthe analysis among all models, a moving average should be chosen because\r\nit had the best accuracy measures (MAPE, MAD, and MSD). For the daily\r\nticket sale, there is not just a trend, but a seasonal pattern within\r\nthe time series. So, the Holt-Winters method did a good job on\r\npredicting ticket sales. With this information, it is fair to assume\r\nthat in the prediction of future movie ticket sales, a moving average\r\nwould be best to determine weekly ticket sales and a Holt-Winters model\r\nwould be best for daily ticket sales. Unfortunately, neither model did a\r\ngreat job of predicting opening day/week sales, but the models steadied\r\nafter that. So, in conclusion the models mentioned above are useful in\r\npredicting ticket sales for summer blockbuster superhero movies, and may\r\nbe useful in predicting sales for all movies; however, the prediction is\r\nlimited to times beyond the opening week.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-08-29T21:38:39-05:00",
    "input_file": {}
  }
]
